{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 1203,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.012468827930174564,
      "grad_norm": 1.4822611808776855,
      "learning_rate": 0.00019933499584372404,
      "loss": 2.8139,
      "step": 5
    },
    {
      "epoch": 0.02493765586034913,
      "grad_norm": 1.8591232299804688,
      "learning_rate": 0.00019850374064837907,
      "loss": 3.0885,
      "step": 10
    },
    {
      "epoch": 0.03740648379052369,
      "grad_norm": 1.7669744491577148,
      "learning_rate": 0.00019767248545303408,
      "loss": 3.4468,
      "step": 15
    },
    {
      "epoch": 0.04987531172069826,
      "grad_norm": 1.9747495651245117,
      "learning_rate": 0.00019684123025768912,
      "loss": 3.1771,
      "step": 20
    },
    {
      "epoch": 0.06234413965087282,
      "grad_norm": 4.034521102905273,
      "learning_rate": 0.00019600997506234415,
      "loss": 2.8412,
      "step": 25
    },
    {
      "epoch": 0.07481296758104738,
      "grad_norm": 3.045631170272827,
      "learning_rate": 0.0001951787198669992,
      "loss": 2.0691,
      "step": 30
    },
    {
      "epoch": 0.08728179551122195,
      "grad_norm": 3.047288179397583,
      "learning_rate": 0.00019434746467165422,
      "loss": 2.5845,
      "step": 35
    },
    {
      "epoch": 0.09975062344139651,
      "grad_norm": 6.318612098693848,
      "learning_rate": 0.00019351620947630923,
      "loss": 2.1663,
      "step": 40
    },
    {
      "epoch": 0.11221945137157108,
      "grad_norm": 3.7762396335601807,
      "learning_rate": 0.00019268495428096427,
      "loss": 2.462,
      "step": 45
    },
    {
      "epoch": 0.12468827930174564,
      "grad_norm": 6.118077754974365,
      "learning_rate": 0.0001918536990856193,
      "loss": 2.5693,
      "step": 50
    },
    {
      "epoch": 0.1371571072319202,
      "grad_norm": 2.8831088542938232,
      "learning_rate": 0.00019102244389027434,
      "loss": 2.0658,
      "step": 55
    },
    {
      "epoch": 0.14962593516209477,
      "grad_norm": 4.854796409606934,
      "learning_rate": 0.00019019118869492935,
      "loss": 2.3441,
      "step": 60
    },
    {
      "epoch": 0.16209476309226933,
      "grad_norm": 3.783867597579956,
      "learning_rate": 0.00018935993349958438,
      "loss": 1.9873,
      "step": 65
    },
    {
      "epoch": 0.1745635910224439,
      "grad_norm": 5.279199600219727,
      "learning_rate": 0.00018852867830423942,
      "loss": 1.8264,
      "step": 70
    },
    {
      "epoch": 0.18703241895261846,
      "grad_norm": 4.2186713218688965,
      "learning_rate": 0.00018769742310889445,
      "loss": 1.8305,
      "step": 75
    },
    {
      "epoch": 0.19950124688279303,
      "grad_norm": 5.163958549499512,
      "learning_rate": 0.00018686616791354946,
      "loss": 1.8211,
      "step": 80
    },
    {
      "epoch": 0.2119700748129676,
      "grad_norm": 4.266040325164795,
      "learning_rate": 0.0001860349127182045,
      "loss": 2.2284,
      "step": 85
    },
    {
      "epoch": 0.22443890274314215,
      "grad_norm": 5.865045070648193,
      "learning_rate": 0.00018520365752285953,
      "loss": 1.9128,
      "step": 90
    },
    {
      "epoch": 0.23690773067331672,
      "grad_norm": 5.717809200286865,
      "learning_rate": 0.00018437240232751457,
      "loss": 1.7463,
      "step": 95
    },
    {
      "epoch": 0.24937655860349128,
      "grad_norm": 5.407817363739014,
      "learning_rate": 0.00018354114713216958,
      "loss": 1.5413,
      "step": 100
    },
    {
      "epoch": 0.26184538653366585,
      "grad_norm": 5.657718181610107,
      "learning_rate": 0.0001827098919368246,
      "loss": 1.7232,
      "step": 105
    },
    {
      "epoch": 0.2743142144638404,
      "grad_norm": 9.167716026306152,
      "learning_rate": 0.00018187863674147965,
      "loss": 1.5122,
      "step": 110
    },
    {
      "epoch": 0.286783042394015,
      "grad_norm": 6.6259050369262695,
      "learning_rate": 0.00018104738154613468,
      "loss": 1.9351,
      "step": 115
    },
    {
      "epoch": 0.29925187032418954,
      "grad_norm": 6.140316963195801,
      "learning_rate": 0.0001802161263507897,
      "loss": 1.835,
      "step": 120
    },
    {
      "epoch": 0.3117206982543641,
      "grad_norm": 4.904848098754883,
      "learning_rate": 0.00017938487115544473,
      "loss": 1.48,
      "step": 125
    },
    {
      "epoch": 0.32418952618453867,
      "grad_norm": 5.337700366973877,
      "learning_rate": 0.00017855361596009976,
      "loss": 1.6498,
      "step": 130
    },
    {
      "epoch": 0.33665835411471323,
      "grad_norm": 5.957918167114258,
      "learning_rate": 0.0001777223607647548,
      "loss": 1.5588,
      "step": 135
    },
    {
      "epoch": 0.3491271820448878,
      "grad_norm": 7.884795665740967,
      "learning_rate": 0.00017689110556940983,
      "loss": 1.5416,
      "step": 140
    },
    {
      "epoch": 0.36159600997506236,
      "grad_norm": 6.403569221496582,
      "learning_rate": 0.00017605985037406484,
      "loss": 1.3458,
      "step": 145
    },
    {
      "epoch": 0.3740648379052369,
      "grad_norm": 6.658194065093994,
      "learning_rate": 0.00017522859517871988,
      "loss": 1.2535,
      "step": 150
    },
    {
      "epoch": 0.3865336658354115,
      "grad_norm": 11.533520698547363,
      "learning_rate": 0.0001743973399833749,
      "loss": 1.6026,
      "step": 155
    },
    {
      "epoch": 0.39900249376558605,
      "grad_norm": 7.426665782928467,
      "learning_rate": 0.00017356608478802995,
      "loss": 1.3085,
      "step": 160
    },
    {
      "epoch": 0.4114713216957606,
      "grad_norm": 13.526792526245117,
      "learning_rate": 0.00017273482959268496,
      "loss": 1.3831,
      "step": 165
    },
    {
      "epoch": 0.4239401496259352,
      "grad_norm": 10.310513496398926,
      "learning_rate": 0.00017190357439734,
      "loss": 1.3281,
      "step": 170
    },
    {
      "epoch": 0.43640897755610975,
      "grad_norm": 6.684528350830078,
      "learning_rate": 0.00017107231920199503,
      "loss": 1.3295,
      "step": 175
    },
    {
      "epoch": 0.4488778054862843,
      "grad_norm": 8.722527503967285,
      "learning_rate": 0.00017024106400665006,
      "loss": 1.5108,
      "step": 180
    },
    {
      "epoch": 0.4613466334164589,
      "grad_norm": 5.279642105102539,
      "learning_rate": 0.00016940980881130507,
      "loss": 1.4022,
      "step": 185
    },
    {
      "epoch": 0.47381546134663344,
      "grad_norm": 5.987054824829102,
      "learning_rate": 0.0001685785536159601,
      "loss": 1.1952,
      "step": 190
    },
    {
      "epoch": 0.486284289276808,
      "grad_norm": 10.907719612121582,
      "learning_rate": 0.00016774729842061514,
      "loss": 1.2407,
      "step": 195
    },
    {
      "epoch": 0.49875311720698257,
      "grad_norm": 6.2720723152160645,
      "learning_rate": 0.00016691604322527018,
      "loss": 0.9857,
      "step": 200
    },
    {
      "epoch": 0.5112219451371571,
      "grad_norm": 7.8223114013671875,
      "learning_rate": 0.00016608478802992519,
      "loss": 1.3497,
      "step": 205
    },
    {
      "epoch": 0.5236907730673317,
      "grad_norm": 5.458625316619873,
      "learning_rate": 0.00016525353283458022,
      "loss": 1.2375,
      "step": 210
    },
    {
      "epoch": 0.5361596009975063,
      "grad_norm": 7.745327949523926,
      "learning_rate": 0.00016442227763923526,
      "loss": 1.5128,
      "step": 215
    },
    {
      "epoch": 0.5486284289276808,
      "grad_norm": 8.8864107131958,
      "learning_rate": 0.0001635910224438903,
      "loss": 1.6021,
      "step": 220
    },
    {
      "epoch": 0.5610972568578554,
      "grad_norm": 9.717578887939453,
      "learning_rate": 0.0001627597672485453,
      "loss": 1.8736,
      "step": 225
    },
    {
      "epoch": 0.57356608478803,
      "grad_norm": 10.402445793151855,
      "learning_rate": 0.00016192851205320034,
      "loss": 1.2288,
      "step": 230
    },
    {
      "epoch": 0.5860349127182045,
      "grad_norm": 7.630560398101807,
      "learning_rate": 0.00016109725685785537,
      "loss": 1.5336,
      "step": 235
    },
    {
      "epoch": 0.5985037406483791,
      "grad_norm": 11.432061195373535,
      "learning_rate": 0.0001602660016625104,
      "loss": 1.2688,
      "step": 240
    },
    {
      "epoch": 0.6109725685785536,
      "grad_norm": 9.257566452026367,
      "learning_rate": 0.00015943474646716544,
      "loss": 1.4867,
      "step": 245
    },
    {
      "epoch": 0.6234413965087282,
      "grad_norm": 7.205962181091309,
      "learning_rate": 0.00015860349127182045,
      "loss": 1.255,
      "step": 250
    },
    {
      "epoch": 0.6359102244389028,
      "grad_norm": 11.077688217163086,
      "learning_rate": 0.0001577722360764755,
      "loss": 1.0273,
      "step": 255
    },
    {
      "epoch": 0.6483790523690773,
      "grad_norm": 8.185164451599121,
      "learning_rate": 0.00015694098088113052,
      "loss": 1.2439,
      "step": 260
    },
    {
      "epoch": 0.6608478802992519,
      "grad_norm": 7.275746822357178,
      "learning_rate": 0.00015610972568578556,
      "loss": 1.4065,
      "step": 265
    },
    {
      "epoch": 0.6733167082294265,
      "grad_norm": 8.140104293823242,
      "learning_rate": 0.00015527847049044057,
      "loss": 1.0731,
      "step": 270
    },
    {
      "epoch": 0.685785536159601,
      "grad_norm": 4.95633602142334,
      "learning_rate": 0.0001544472152950956,
      "loss": 1.3364,
      "step": 275
    },
    {
      "epoch": 0.6982543640897756,
      "grad_norm": 15.492871284484863,
      "learning_rate": 0.00015361596009975064,
      "loss": 1.6708,
      "step": 280
    },
    {
      "epoch": 0.7107231920199502,
      "grad_norm": 5.840738773345947,
      "learning_rate": 0.00015278470490440567,
      "loss": 1.4059,
      "step": 285
    },
    {
      "epoch": 0.7231920199501247,
      "grad_norm": 6.028252601623535,
      "learning_rate": 0.00015195344970906068,
      "loss": 1.1458,
      "step": 290
    },
    {
      "epoch": 0.7356608478802993,
      "grad_norm": 7.447889804840088,
      "learning_rate": 0.00015112219451371572,
      "loss": 1.5297,
      "step": 295
    },
    {
      "epoch": 0.7481296758104738,
      "grad_norm": 5.454870700836182,
      "learning_rate": 0.00015029093931837075,
      "loss": 0.6857,
      "step": 300
    },
    {
      "epoch": 0.7605985037406484,
      "grad_norm": 10.189830780029297,
      "learning_rate": 0.0001494596841230258,
      "loss": 0.6181,
      "step": 305
    },
    {
      "epoch": 0.773067331670823,
      "grad_norm": 19.419544219970703,
      "learning_rate": 0.0001486284289276808,
      "loss": 1.4323,
      "step": 310
    },
    {
      "epoch": 0.7855361596009975,
      "grad_norm": 5.600466728210449,
      "learning_rate": 0.00014779717373233583,
      "loss": 1.0015,
      "step": 315
    },
    {
      "epoch": 0.7980049875311721,
      "grad_norm": 12.9729642868042,
      "learning_rate": 0.00014696591853699087,
      "loss": 1.016,
      "step": 320
    },
    {
      "epoch": 0.8104738154613467,
      "grad_norm": 4.632493495941162,
      "learning_rate": 0.0001461346633416459,
      "loss": 0.964,
      "step": 325
    },
    {
      "epoch": 0.8229426433915212,
      "grad_norm": 11.480193138122559,
      "learning_rate": 0.0001453034081463009,
      "loss": 2.063,
      "step": 330
    },
    {
      "epoch": 0.8354114713216958,
      "grad_norm": 7.389138221740723,
      "learning_rate": 0.00014447215295095595,
      "loss": 0.8051,
      "step": 335
    },
    {
      "epoch": 0.8478802992518704,
      "grad_norm": 7.292203903198242,
      "learning_rate": 0.00014364089775561098,
      "loss": 1.2166,
      "step": 340
    },
    {
      "epoch": 0.8603491271820449,
      "grad_norm": 6.159506797790527,
      "learning_rate": 0.00014280964256026602,
      "loss": 1.427,
      "step": 345
    },
    {
      "epoch": 0.8728179551122195,
      "grad_norm": 12.199625968933105,
      "learning_rate": 0.00014197838736492105,
      "loss": 1.499,
      "step": 350
    },
    {
      "epoch": 0.885286783042394,
      "grad_norm": 4.525938987731934,
      "learning_rate": 0.00014114713216957606,
      "loss": 0.7902,
      "step": 355
    },
    {
      "epoch": 0.8977556109725686,
      "grad_norm": 4.982956409454346,
      "learning_rate": 0.0001403158769742311,
      "loss": 1.1197,
      "step": 360
    },
    {
      "epoch": 0.9102244389027432,
      "grad_norm": 8.978094100952148,
      "learning_rate": 0.00013948462177888613,
      "loss": 1.0989,
      "step": 365
    },
    {
      "epoch": 0.9226932668329177,
      "grad_norm": 6.642364025115967,
      "learning_rate": 0.00013865336658354117,
      "loss": 0.8005,
      "step": 370
    },
    {
      "epoch": 0.9351620947630923,
      "grad_norm": 11.03065299987793,
      "learning_rate": 0.00013782211138819618,
      "loss": 1.2428,
      "step": 375
    },
    {
      "epoch": 0.9476309226932669,
      "grad_norm": 4.287650108337402,
      "learning_rate": 0.0001369908561928512,
      "loss": 0.4673,
      "step": 380
    },
    {
      "epoch": 0.9600997506234414,
      "grad_norm": 7.7606096267700195,
      "learning_rate": 0.00013615960099750625,
      "loss": 1.1893,
      "step": 385
    },
    {
      "epoch": 0.972568578553616,
      "grad_norm": 7.302929878234863,
      "learning_rate": 0.00013532834580216128,
      "loss": 1.4178,
      "step": 390
    },
    {
      "epoch": 0.9850374064837906,
      "grad_norm": 20.277379989624023,
      "learning_rate": 0.0001344970906068163,
      "loss": 0.6979,
      "step": 395
    },
    {
      "epoch": 0.9975062344139651,
      "grad_norm": 8.400141716003418,
      "learning_rate": 0.00013366583541147133,
      "loss": 0.9922,
      "step": 400
    },
    {
      "epoch": 1.0099750623441397,
      "grad_norm": 5.224287986755371,
      "learning_rate": 0.00013283458021612636,
      "loss": 0.8872,
      "step": 405
    },
    {
      "epoch": 1.0224438902743143,
      "grad_norm": 7.153343677520752,
      "learning_rate": 0.0001320033250207814,
      "loss": 0.4958,
      "step": 410
    },
    {
      "epoch": 1.0349127182044888,
      "grad_norm": 16.67095184326172,
      "learning_rate": 0.0001311720698254364,
      "loss": 0.7939,
      "step": 415
    },
    {
      "epoch": 1.0473815461346634,
      "grad_norm": 5.528717041015625,
      "learning_rate": 0.00013034081463009144,
      "loss": 0.6966,
      "step": 420
    },
    {
      "epoch": 1.059850374064838,
      "grad_norm": 4.924174785614014,
      "learning_rate": 0.00012950955943474648,
      "loss": 0.5183,
      "step": 425
    },
    {
      "epoch": 1.0723192019950125,
      "grad_norm": 11.933476448059082,
      "learning_rate": 0.0001286783042394015,
      "loss": 1.1299,
      "step": 430
    },
    {
      "epoch": 1.084788029925187,
      "grad_norm": 3.4326095581054688,
      "learning_rate": 0.00012784704904405652,
      "loss": 0.5716,
      "step": 435
    },
    {
      "epoch": 1.0972568578553616,
      "grad_norm": 2.7287588119506836,
      "learning_rate": 0.00012701579384871156,
      "loss": 0.9313,
      "step": 440
    },
    {
      "epoch": 1.1097256857855362,
      "grad_norm": 17.002941131591797,
      "learning_rate": 0.0001261845386533666,
      "loss": 0.8375,
      "step": 445
    },
    {
      "epoch": 1.1221945137157108,
      "grad_norm": 7.573513031005859,
      "learning_rate": 0.00012535328345802163,
      "loss": 1.3861,
      "step": 450
    },
    {
      "epoch": 1.1346633416458853,
      "grad_norm": 3.8387715816497803,
      "learning_rate": 0.00012452202826267666,
      "loss": 0.3316,
      "step": 455
    },
    {
      "epoch": 1.14713216957606,
      "grad_norm": 14.725857734680176,
      "learning_rate": 0.00012369077306733167,
      "loss": 1.3283,
      "step": 460
    },
    {
      "epoch": 1.1596009975062345,
      "grad_norm": 6.894521713256836,
      "learning_rate": 0.0001228595178719867,
      "loss": 1.0553,
      "step": 465
    },
    {
      "epoch": 1.172069825436409,
      "grad_norm": 11.34838581085205,
      "learning_rate": 0.00012202826267664174,
      "loss": 1.0301,
      "step": 470
    },
    {
      "epoch": 1.1845386533665836,
      "grad_norm": 1.6239951848983765,
      "learning_rate": 0.00012119700748129676,
      "loss": 0.5822,
      "step": 475
    },
    {
      "epoch": 1.1970074812967582,
      "grad_norm": 9.200233459472656,
      "learning_rate": 0.0001203657522859518,
      "loss": 1.1691,
      "step": 480
    },
    {
      "epoch": 1.2094763092269327,
      "grad_norm": 4.7877912521362305,
      "learning_rate": 0.00011953449709060682,
      "loss": 0.6776,
      "step": 485
    },
    {
      "epoch": 1.2219451371571073,
      "grad_norm": 31.97602081298828,
      "learning_rate": 0.00011870324189526186,
      "loss": 1.1225,
      "step": 490
    },
    {
      "epoch": 1.2344139650872819,
      "grad_norm": 8.078327178955078,
      "learning_rate": 0.00011787198669991688,
      "loss": 0.792,
      "step": 495
    },
    {
      "epoch": 1.2468827930174564,
      "grad_norm": 4.672518253326416,
      "learning_rate": 0.00011704073150457191,
      "loss": 0.7271,
      "step": 500
    },
    {
      "epoch": 1.259351620947631,
      "grad_norm": 4.4684672355651855,
      "learning_rate": 0.00011620947630922693,
      "loss": 0.6681,
      "step": 505
    },
    {
      "epoch": 1.2718204488778055,
      "grad_norm": 17.53610610961914,
      "learning_rate": 0.00011537822111388197,
      "loss": 0.4455,
      "step": 510
    },
    {
      "epoch": 1.28428927680798,
      "grad_norm": 6.65346622467041,
      "learning_rate": 0.00011454696591853699,
      "loss": 0.5857,
      "step": 515
    },
    {
      "epoch": 1.2967581047381547,
      "grad_norm": 5.474069118499756,
      "learning_rate": 0.00011371571072319203,
      "loss": 0.7854,
      "step": 520
    },
    {
      "epoch": 1.3092269326683292,
      "grad_norm": 4.554190158843994,
      "learning_rate": 0.00011288445552784705,
      "loss": 0.587,
      "step": 525
    },
    {
      "epoch": 1.3216957605985038,
      "grad_norm": 6.211636066436768,
      "learning_rate": 0.00011205320033250209,
      "loss": 0.9837,
      "step": 530
    },
    {
      "epoch": 1.3341645885286784,
      "grad_norm": 2.3341224193573,
      "learning_rate": 0.00011122194513715711,
      "loss": 0.9712,
      "step": 535
    },
    {
      "epoch": 1.346633416458853,
      "grad_norm": 9.237048149108887,
      "learning_rate": 0.00011039068994181214,
      "loss": 0.708,
      "step": 540
    },
    {
      "epoch": 1.3591022443890275,
      "grad_norm": 14.476147651672363,
      "learning_rate": 0.00010955943474646716,
      "loss": 0.5741,
      "step": 545
    },
    {
      "epoch": 1.371571072319202,
      "grad_norm": 24.638036727905273,
      "learning_rate": 0.0001087281795511222,
      "loss": 0.7892,
      "step": 550
    },
    {
      "epoch": 1.3840399002493766,
      "grad_norm": 15.700482368469238,
      "learning_rate": 0.00010789692435577724,
      "loss": 0.9085,
      "step": 555
    },
    {
      "epoch": 1.3965087281795512,
      "grad_norm": 12.291950225830078,
      "learning_rate": 0.00010706566916043226,
      "loss": 0.7083,
      "step": 560
    },
    {
      "epoch": 1.4089775561097257,
      "grad_norm": 5.437875270843506,
      "learning_rate": 0.00010623441396508729,
      "loss": 0.2436,
      "step": 565
    },
    {
      "epoch": 1.4214463840399003,
      "grad_norm": 2.788569450378418,
      "learning_rate": 0.00010540315876974231,
      "loss": 0.5002,
      "step": 570
    },
    {
      "epoch": 1.4339152119700749,
      "grad_norm": 11.846182823181152,
      "learning_rate": 0.00010457190357439735,
      "loss": 1.5391,
      "step": 575
    },
    {
      "epoch": 1.4463840399002494,
      "grad_norm": 3.629216432571411,
      "learning_rate": 0.00010374064837905237,
      "loss": 1.1064,
      "step": 580
    },
    {
      "epoch": 1.458852867830424,
      "grad_norm": 3.7749390602111816,
      "learning_rate": 0.00010290939318370741,
      "loss": 0.6579,
      "step": 585
    },
    {
      "epoch": 1.4713216957605986,
      "grad_norm": 9.386958122253418,
      "learning_rate": 0.00010207813798836243,
      "loss": 0.8875,
      "step": 590
    },
    {
      "epoch": 1.4837905236907731,
      "grad_norm": 5.410234451293945,
      "learning_rate": 0.00010124688279301747,
      "loss": 0.5764,
      "step": 595
    },
    {
      "epoch": 1.4962593516209477,
      "grad_norm": 13.34489917755127,
      "learning_rate": 0.00010041562759767249,
      "loss": 0.3831,
      "step": 600
    },
    {
      "epoch": 1.508728179551122,
      "grad_norm": 2.0570473670959473,
      "learning_rate": 9.958437240232752e-05,
      "loss": 1.3486,
      "step": 605
    },
    {
      "epoch": 1.5211970074812968,
      "grad_norm": 11.154268264770508,
      "learning_rate": 9.875311720698254e-05,
      "loss": 1.0814,
      "step": 610
    },
    {
      "epoch": 1.5336658354114712,
      "grad_norm": 6.060518741607666,
      "learning_rate": 9.792186201163758e-05,
      "loss": 0.6593,
      "step": 615
    },
    {
      "epoch": 1.546134663341646,
      "grad_norm": 3.4206738471984863,
      "learning_rate": 9.70906068162926e-05,
      "loss": 0.7238,
      "step": 620
    },
    {
      "epoch": 1.5586034912718203,
      "grad_norm": 8.320548057556152,
      "learning_rate": 9.625935162094764e-05,
      "loss": 1.0072,
      "step": 625
    },
    {
      "epoch": 1.571072319201995,
      "grad_norm": 8.615962028503418,
      "learning_rate": 9.542809642560266e-05,
      "loss": 1.4502,
      "step": 630
    },
    {
      "epoch": 1.5835411471321694,
      "grad_norm": 3.112823963165283,
      "learning_rate": 9.45968412302577e-05,
      "loss": 0.2685,
      "step": 635
    },
    {
      "epoch": 1.5960099750623442,
      "grad_norm": 16.677791595458984,
      "learning_rate": 9.376558603491272e-05,
      "loss": 0.5236,
      "step": 640
    },
    {
      "epoch": 1.6084788029925186,
      "grad_norm": 9.236804008483887,
      "learning_rate": 9.293433083956775e-05,
      "loss": 0.6995,
      "step": 645
    },
    {
      "epoch": 1.6209476309226933,
      "grad_norm": 2.6396491527557373,
      "learning_rate": 9.210307564422277e-05,
      "loss": 0.7174,
      "step": 650
    },
    {
      "epoch": 1.6334164588528677,
      "grad_norm": 4.934444427490234,
      "learning_rate": 9.127182044887781e-05,
      "loss": 0.6558,
      "step": 655
    },
    {
      "epoch": 1.6458852867830425,
      "grad_norm": 6.384671688079834,
      "learning_rate": 9.044056525353284e-05,
      "loss": 0.391,
      "step": 660
    },
    {
      "epoch": 1.6583541147132168,
      "grad_norm": 3.546959400177002,
      "learning_rate": 8.960931005818787e-05,
      "loss": 1.0335,
      "step": 665
    },
    {
      "epoch": 1.6708229426433916,
      "grad_norm": 1.874607801437378,
      "learning_rate": 8.87780548628429e-05,
      "loss": 0.3252,
      "step": 670
    },
    {
      "epoch": 1.683291770573566,
      "grad_norm": 3.7136034965515137,
      "learning_rate": 8.794679966749792e-05,
      "loss": 0.2028,
      "step": 675
    },
    {
      "epoch": 1.6957605985037407,
      "grad_norm": 9.019113540649414,
      "learning_rate": 8.711554447215296e-05,
      "loss": 0.7543,
      "step": 680
    },
    {
      "epoch": 1.708229426433915,
      "grad_norm": 4.944094657897949,
      "learning_rate": 8.628428927680798e-05,
      "loss": 0.6782,
      "step": 685
    },
    {
      "epoch": 1.7206982543640899,
      "grad_norm": 8.697751998901367,
      "learning_rate": 8.545303408146302e-05,
      "loss": 0.7064,
      "step": 690
    },
    {
      "epoch": 1.7331670822942642,
      "grad_norm": 4.040490627288818,
      "learning_rate": 8.462177888611804e-05,
      "loss": 0.4629,
      "step": 695
    },
    {
      "epoch": 1.745635910224439,
      "grad_norm": 10.241497993469238,
      "learning_rate": 8.379052369077307e-05,
      "loss": 1.0994,
      "step": 700
    },
    {
      "epoch": 1.7581047381546133,
      "grad_norm": 9.783232688903809,
      "learning_rate": 8.29592684954281e-05,
      "loss": 0.523,
      "step": 705
    },
    {
      "epoch": 1.770573566084788,
      "grad_norm": 8.635810852050781,
      "learning_rate": 8.212801330008313e-05,
      "loss": 0.5098,
      "step": 710
    },
    {
      "epoch": 1.7830423940149625,
      "grad_norm": 4.68344259262085,
      "learning_rate": 8.129675810473815e-05,
      "loss": 0.6502,
      "step": 715
    },
    {
      "epoch": 1.7955112219451372,
      "grad_norm": 3.5723862648010254,
      "learning_rate": 8.046550290939319e-05,
      "loss": 0.2706,
      "step": 720
    },
    {
      "epoch": 1.8079800498753116,
      "grad_norm": 3.9368433952331543,
      "learning_rate": 7.963424771404821e-05,
      "loss": 0.238,
      "step": 725
    },
    {
      "epoch": 1.8204488778054864,
      "grad_norm": 1.4475622177124023,
      "learning_rate": 7.880299251870325e-05,
      "loss": 0.5009,
      "step": 730
    },
    {
      "epoch": 1.8329177057356607,
      "grad_norm": 13.333111763000488,
      "learning_rate": 7.797173732335827e-05,
      "loss": 0.4972,
      "step": 735
    },
    {
      "epoch": 1.8453865336658355,
      "grad_norm": 4.280488014221191,
      "learning_rate": 7.71404821280133e-05,
      "loss": 0.5493,
      "step": 740
    },
    {
      "epoch": 1.8578553615960098,
      "grad_norm": 12.225491523742676,
      "learning_rate": 7.630922693266833e-05,
      "loss": 0.8533,
      "step": 745
    },
    {
      "epoch": 1.8703241895261846,
      "grad_norm": 10.019986152648926,
      "learning_rate": 7.547797173732336e-05,
      "loss": 0.7197,
      "step": 750
    },
    {
      "epoch": 1.882793017456359,
      "grad_norm": 3.086660623550415,
      "learning_rate": 7.464671654197838e-05,
      "loss": 0.3204,
      "step": 755
    },
    {
      "epoch": 1.8952618453865338,
      "grad_norm": 5.259582042694092,
      "learning_rate": 7.381546134663342e-05,
      "loss": 0.3617,
      "step": 760
    },
    {
      "epoch": 1.907730673316708,
      "grad_norm": 3.339033365249634,
      "learning_rate": 7.298420615128845e-05,
      "loss": 0.7596,
      "step": 765
    },
    {
      "epoch": 1.9201995012468829,
      "grad_norm": 9.373282432556152,
      "learning_rate": 7.215295095594348e-05,
      "loss": 1.1277,
      "step": 770
    },
    {
      "epoch": 1.9326683291770572,
      "grad_norm": 4.590565204620361,
      "learning_rate": 7.132169576059851e-05,
      "loss": 0.319,
      "step": 775
    },
    {
      "epoch": 1.945137157107232,
      "grad_norm": 13.26193904876709,
      "learning_rate": 7.049044056525353e-05,
      "loss": 0.9563,
      "step": 780
    },
    {
      "epoch": 1.9576059850374063,
      "grad_norm": 2.807391405105591,
      "learning_rate": 6.965918536990857e-05,
      "loss": 0.4909,
      "step": 785
    },
    {
      "epoch": 1.9700748129675811,
      "grad_norm": 5.262772083282471,
      "learning_rate": 6.882793017456359e-05,
      "loss": 0.7229,
      "step": 790
    },
    {
      "epoch": 1.9825436408977555,
      "grad_norm": 3.5258748531341553,
      "learning_rate": 6.799667497921863e-05,
      "loss": 0.2707,
      "step": 795
    },
    {
      "epoch": 1.9950124688279303,
      "grad_norm": 2.0000720024108887,
      "learning_rate": 6.716541978387365e-05,
      "loss": 0.7767,
      "step": 800
    },
    {
      "epoch": 2.0074812967581046,
      "grad_norm": 12.144359588623047,
      "learning_rate": 6.633416458852868e-05,
      "loss": 1.045,
      "step": 805
    },
    {
      "epoch": 2.0199501246882794,
      "grad_norm": 3.2890326976776123,
      "learning_rate": 6.55029093931837e-05,
      "loss": 0.2518,
      "step": 810
    },
    {
      "epoch": 2.0324189526184537,
      "grad_norm": 3.7765536308288574,
      "learning_rate": 6.467165419783874e-05,
      "loss": 0.3776,
      "step": 815
    },
    {
      "epoch": 2.0448877805486285,
      "grad_norm": 2.7336742877960205,
      "learning_rate": 6.384039900249376e-05,
      "loss": 0.221,
      "step": 820
    },
    {
      "epoch": 2.057356608478803,
      "grad_norm": 2.273252010345459,
      "learning_rate": 6.30091438071488e-05,
      "loss": 0.2239,
      "step": 825
    },
    {
      "epoch": 2.0698254364089776,
      "grad_norm": 4.704333782196045,
      "learning_rate": 6.217788861180382e-05,
      "loss": 0.8639,
      "step": 830
    },
    {
      "epoch": 2.082294264339152,
      "grad_norm": 3.878225326538086,
      "learning_rate": 6.134663341645886e-05,
      "loss": 0.8926,
      "step": 835
    },
    {
      "epoch": 2.0947630922693268,
      "grad_norm": 9.86694049835205,
      "learning_rate": 6.0515378221113885e-05,
      "loss": 0.6533,
      "step": 840
    },
    {
      "epoch": 2.107231920199501,
      "grad_norm": 4.635146141052246,
      "learning_rate": 5.9684123025768914e-05,
      "loss": 0.283,
      "step": 845
    },
    {
      "epoch": 2.119700748129676,
      "grad_norm": 4.521968841552734,
      "learning_rate": 5.885286783042394e-05,
      "loss": 0.6077,
      "step": 850
    },
    {
      "epoch": 2.1321695760598502,
      "grad_norm": 5.229687213897705,
      "learning_rate": 5.802161263507897e-05,
      "loss": 0.3992,
      "step": 855
    },
    {
      "epoch": 2.144638403990025,
      "grad_norm": 11.832067489624023,
      "learning_rate": 5.7190357439734e-05,
      "loss": 0.9201,
      "step": 860
    },
    {
      "epoch": 2.1571072319201994,
      "grad_norm": 10.848637580871582,
      "learning_rate": 5.635910224438903e-05,
      "loss": 0.9121,
      "step": 865
    },
    {
      "epoch": 2.169576059850374,
      "grad_norm": 5.228872299194336,
      "learning_rate": 5.552784704904406e-05,
      "loss": 1.1781,
      "step": 870
    },
    {
      "epoch": 2.1820448877805485,
      "grad_norm": 1.8813714981079102,
      "learning_rate": 5.4696591853699086e-05,
      "loss": 0.4869,
      "step": 875
    },
    {
      "epoch": 2.1945137157107233,
      "grad_norm": 12.33902359008789,
      "learning_rate": 5.3865336658354115e-05,
      "loss": 0.334,
      "step": 880
    },
    {
      "epoch": 2.2069825436408976,
      "grad_norm": 5.662548542022705,
      "learning_rate": 5.303408146300914e-05,
      "loss": 0.1993,
      "step": 885
    },
    {
      "epoch": 2.2194513715710724,
      "grad_norm": 1.3314753770828247,
      "learning_rate": 5.220282626766417e-05,
      "loss": 0.7529,
      "step": 890
    },
    {
      "epoch": 2.2319201995012468,
      "grad_norm": 7.790630340576172,
      "learning_rate": 5.13715710723192e-05,
      "loss": 0.681,
      "step": 895
    },
    {
      "epoch": 2.2443890274314215,
      "grad_norm": 3.4530622959136963,
      "learning_rate": 5.054031587697423e-05,
      "loss": 0.4029,
      "step": 900
    },
    {
      "epoch": 2.256857855361596,
      "grad_norm": 5.2517781257629395,
      "learning_rate": 4.970906068162926e-05,
      "loss": 0.7817,
      "step": 905
    },
    {
      "epoch": 2.2693266832917707,
      "grad_norm": 10.859539985656738,
      "learning_rate": 4.887780548628429e-05,
      "loss": 0.6846,
      "step": 910
    },
    {
      "epoch": 2.281795511221945,
      "grad_norm": 7.073276996612549,
      "learning_rate": 4.8046550290939315e-05,
      "loss": 0.3086,
      "step": 915
    },
    {
      "epoch": 2.29426433915212,
      "grad_norm": 7.097685813903809,
      "learning_rate": 4.7215295095594344e-05,
      "loss": 0.7235,
      "step": 920
    },
    {
      "epoch": 2.306733167082294,
      "grad_norm": 2.0817816257476807,
      "learning_rate": 4.638403990024938e-05,
      "loss": 0.3941,
      "step": 925
    },
    {
      "epoch": 2.319201995012469,
      "grad_norm": 3.2857065200805664,
      "learning_rate": 4.555278470490441e-05,
      "loss": 0.6067,
      "step": 930
    },
    {
      "epoch": 2.3316708229426433,
      "grad_norm": 12.624651908874512,
      "learning_rate": 4.472152950955944e-05,
      "loss": 0.8219,
      "step": 935
    },
    {
      "epoch": 2.344139650872818,
      "grad_norm": 2.162404775619507,
      "learning_rate": 4.3890274314214466e-05,
      "loss": 1.1538,
      "step": 940
    },
    {
      "epoch": 2.3566084788029924,
      "grad_norm": 12.117081642150879,
      "learning_rate": 4.3059019118869494e-05,
      "loss": 1.1453,
      "step": 945
    },
    {
      "epoch": 2.369077306733167,
      "grad_norm": 1.106994867324829,
      "learning_rate": 4.222776392352452e-05,
      "loss": 0.213,
      "step": 950
    },
    {
      "epoch": 2.3815461346633415,
      "grad_norm": 14.26935863494873,
      "learning_rate": 4.139650872817955e-05,
      "loss": 0.5417,
      "step": 955
    },
    {
      "epoch": 2.3940149625935163,
      "grad_norm": 5.6926751136779785,
      "learning_rate": 4.056525353283458e-05,
      "loss": 0.4855,
      "step": 960
    },
    {
      "epoch": 2.4064837905236907,
      "grad_norm": 6.9386091232299805,
      "learning_rate": 3.973399833748961e-05,
      "loss": 0.3234,
      "step": 965
    },
    {
      "epoch": 2.4189526184538654,
      "grad_norm": 7.681638240814209,
      "learning_rate": 3.890274314214464e-05,
      "loss": 0.2832,
      "step": 970
    },
    {
      "epoch": 2.43142144638404,
      "grad_norm": 11.909673690795898,
      "learning_rate": 3.8071487946799667e-05,
      "loss": 1.0306,
      "step": 975
    },
    {
      "epoch": 2.4438902743142146,
      "grad_norm": 2.8538808822631836,
      "learning_rate": 3.72402327514547e-05,
      "loss": 0.5752,
      "step": 980
    },
    {
      "epoch": 2.456359102244389,
      "grad_norm": 2.475850820541382,
      "learning_rate": 3.640897755610973e-05,
      "loss": 0.3199,
      "step": 985
    },
    {
      "epoch": 2.4688279301745637,
      "grad_norm": 1.7395631074905396,
      "learning_rate": 3.557772236076476e-05,
      "loss": 0.3849,
      "step": 990
    },
    {
      "epoch": 2.481296758104738,
      "grad_norm": 4.673742294311523,
      "learning_rate": 3.474646716541979e-05,
      "loss": 0.6316,
      "step": 995
    },
    {
      "epoch": 2.493765586034913,
      "grad_norm": 8.279725074768066,
      "learning_rate": 3.391521197007482e-05,
      "loss": 0.9546,
      "step": 1000
    },
    {
      "epoch": 2.506234413965087,
      "grad_norm": 9.981765747070312,
      "learning_rate": 3.3083956774729846e-05,
      "loss": 0.3444,
      "step": 1005
    },
    {
      "epoch": 2.518703241895262,
      "grad_norm": 2.4713950157165527,
      "learning_rate": 3.2252701579384874e-05,
      "loss": 0.2239,
      "step": 1010
    },
    {
      "epoch": 2.5311720698254363,
      "grad_norm": 9.18397331237793,
      "learning_rate": 3.14214463840399e-05,
      "loss": 0.8304,
      "step": 1015
    },
    {
      "epoch": 2.543640897755611,
      "grad_norm": 8.618400573730469,
      "learning_rate": 3.059019118869493e-05,
      "loss": 0.396,
      "step": 1020
    },
    {
      "epoch": 2.5561097256857854,
      "grad_norm": 4.915675640106201,
      "learning_rate": 2.9758935993349964e-05,
      "loss": 0.2122,
      "step": 1025
    },
    {
      "epoch": 2.56857855361596,
      "grad_norm": 13.25020694732666,
      "learning_rate": 2.8927680798004992e-05,
      "loss": 0.7006,
      "step": 1030
    },
    {
      "epoch": 2.5810473815461346,
      "grad_norm": 5.4040069580078125,
      "learning_rate": 2.809642560266002e-05,
      "loss": 0.8051,
      "step": 1035
    },
    {
      "epoch": 2.5935162094763093,
      "grad_norm": 4.7422285079956055,
      "learning_rate": 2.726517040731505e-05,
      "loss": 0.6476,
      "step": 1040
    },
    {
      "epoch": 2.6059850374064837,
      "grad_norm": 2.4668281078338623,
      "learning_rate": 2.643391521197008e-05,
      "loss": 0.4369,
      "step": 1045
    },
    {
      "epoch": 2.6184538653366585,
      "grad_norm": 4.1724534034729,
      "learning_rate": 2.5602660016625107e-05,
      "loss": 0.2039,
      "step": 1050
    },
    {
      "epoch": 2.630922693266833,
      "grad_norm": 11.9324951171875,
      "learning_rate": 2.4771404821280133e-05,
      "loss": 0.7151,
      "step": 1055
    },
    {
      "epoch": 2.6433915211970076,
      "grad_norm": 2.848126173019409,
      "learning_rate": 2.394014962593516e-05,
      "loss": 0.2087,
      "step": 1060
    },
    {
      "epoch": 2.655860349127182,
      "grad_norm": 2.8783035278320312,
      "learning_rate": 2.310889443059019e-05,
      "loss": 0.2135,
      "step": 1065
    },
    {
      "epoch": 2.6683291770573567,
      "grad_norm": 4.728626251220703,
      "learning_rate": 2.2277639235245222e-05,
      "loss": 0.3283,
      "step": 1070
    },
    {
      "epoch": 2.680798004987531,
      "grad_norm": 10.4381103515625,
      "learning_rate": 2.144638403990025e-05,
      "loss": 0.7501,
      "step": 1075
    },
    {
      "epoch": 2.693266832917706,
      "grad_norm": 9.926336288452148,
      "learning_rate": 2.061512884455528e-05,
      "loss": 0.7433,
      "step": 1080
    },
    {
      "epoch": 2.70573566084788,
      "grad_norm": 5.997721195220947,
      "learning_rate": 1.9783873649210308e-05,
      "loss": 0.2668,
      "step": 1085
    },
    {
      "epoch": 2.718204488778055,
      "grad_norm": 10.046333312988281,
      "learning_rate": 1.8952618453865337e-05,
      "loss": 0.7541,
      "step": 1090
    },
    {
      "epoch": 2.7306733167082293,
      "grad_norm": 9.775362968444824,
      "learning_rate": 1.8121363258520366e-05,
      "loss": 0.5999,
      "step": 1095
    },
    {
      "epoch": 2.743142144638404,
      "grad_norm": 5.199692726135254,
      "learning_rate": 1.7290108063175394e-05,
      "loss": 0.8302,
      "step": 1100
    },
    {
      "epoch": 2.7556109725685785,
      "grad_norm": 5.561595439910889,
      "learning_rate": 1.6458852867830423e-05,
      "loss": 0.5462,
      "step": 1105
    },
    {
      "epoch": 2.7680798004987532,
      "grad_norm": 3.8303639888763428,
      "learning_rate": 1.562759767248545e-05,
      "loss": 0.3101,
      "step": 1110
    },
    {
      "epoch": 2.7805486284289276,
      "grad_norm": 25.432785034179688,
      "learning_rate": 1.4796342477140482e-05,
      "loss": 0.5936,
      "step": 1115
    },
    {
      "epoch": 2.7930174563591024,
      "grad_norm": 2.3341174125671387,
      "learning_rate": 1.396508728179551e-05,
      "loss": 0.9125,
      "step": 1120
    },
    {
      "epoch": 2.8054862842892767,
      "grad_norm": 10.05880355834961,
      "learning_rate": 1.313383208645054e-05,
      "loss": 0.3447,
      "step": 1125
    },
    {
      "epoch": 2.8179551122194515,
      "grad_norm": 2.754117727279663,
      "learning_rate": 1.230257689110557e-05,
      "loss": 0.259,
      "step": 1130
    },
    {
      "epoch": 2.830423940149626,
      "grad_norm": 11.979742050170898,
      "learning_rate": 1.1471321695760599e-05,
      "loss": 0.7556,
      "step": 1135
    },
    {
      "epoch": 2.8428927680798006,
      "grad_norm": 7.571933746337891,
      "learning_rate": 1.0640066500415629e-05,
      "loss": 0.4434,
      "step": 1140
    },
    {
      "epoch": 2.855361596009975,
      "grad_norm": 12.836814880371094,
      "learning_rate": 9.808811305070658e-06,
      "loss": 0.4879,
      "step": 1145
    },
    {
      "epoch": 2.8678304239401498,
      "grad_norm": 5.152442455291748,
      "learning_rate": 8.977556109725686e-06,
      "loss": 0.7306,
      "step": 1150
    },
    {
      "epoch": 2.880299251870324,
      "grad_norm": 8.436771392822266,
      "learning_rate": 8.146300914380715e-06,
      "loss": 0.348,
      "step": 1155
    },
    {
      "epoch": 2.892768079800499,
      "grad_norm": 4.029412746429443,
      "learning_rate": 7.3150457190357446e-06,
      "loss": 0.4471,
      "step": 1160
    },
    {
      "epoch": 2.9052369077306732,
      "grad_norm": 7.67327356338501,
      "learning_rate": 6.483790523690773e-06,
      "loss": 0.315,
      "step": 1165
    },
    {
      "epoch": 2.917705735660848,
      "grad_norm": 2.232900619506836,
      "learning_rate": 5.652535328345803e-06,
      "loss": 0.3729,
      "step": 1170
    },
    {
      "epoch": 2.9301745635910224,
      "grad_norm": 20.975481033325195,
      "learning_rate": 4.8212801330008315e-06,
      "loss": 1.0336,
      "step": 1175
    },
    {
      "epoch": 2.942643391521197,
      "grad_norm": 5.822512626647949,
      "learning_rate": 3.99002493765586e-06,
      "loss": 0.2012,
      "step": 1180
    },
    {
      "epoch": 2.9551122194513715,
      "grad_norm": 20.692800521850586,
      "learning_rate": 3.1587697423108893e-06,
      "loss": 1.0676,
      "step": 1185
    },
    {
      "epoch": 2.9675810473815463,
      "grad_norm": 9.63231372833252,
      "learning_rate": 2.327514546965919e-06,
      "loss": 0.7798,
      "step": 1190
    },
    {
      "epoch": 2.9800498753117206,
      "grad_norm": 4.804447650909424,
      "learning_rate": 1.4962593516209476e-06,
      "loss": 0.6655,
      "step": 1195
    },
    {
      "epoch": 2.9925187032418954,
      "grad_norm": 3.995849847793579,
      "learning_rate": 6.650041562759768e-07,
      "loss": 0.4831,
      "step": 1200
    }
  ],
  "logging_steps": 5,
  "max_steps": 1203,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.026131006324736e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
